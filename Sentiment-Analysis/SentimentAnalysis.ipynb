{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Rewritten sentiment-analysis-model.ipynb\n",
    "\n",
    "# ---\n",
    "# ## Sentiment Analysis with Deep Learning and Baseline Models\n",
    "#\n",
    "# This notebook explores sentiment analysis using both a deep learning approach with a Convolutional Neural Network (CNN) and a traditional machine learning model (Random Forest) as a baseline.\n",
    "#\n",
    "# ### Section 1: Data Loading and Preprocessing\n",
    "#\n",
    "# First, we'll load the dataset and preprocess the text data.\n",
    "# We'll use NLTK for text cleaning, a common and powerful library for NLP tasks.\n",
    "# ---\n",
    "\n",
    "# #### Import Libraries\n",
    "# Inspired by: Course_1_-_Quickstart.ipynb for the basic imports (pandas, numpy, matplotlib)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ---\n",
    "# #### Task 1: Loading the Data\n",
    "#\n",
    "# We'll load the data from the text file into a pandas DataFrame.\n",
    "# Inspired by: Course_1_-_Quickstart.ipynb for general data loading with pandas.\n",
    "# ---\n",
    "\n",
    "def load_sentiment_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads sentiment data from a file into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "    data = [line.strip().split('@') for line in lines]\n",
    "    return pd.DataFrame(data, columns=['text', 'sentiment'])\n",
    "\n",
    "# Load the dataset\n",
    "data_df = load_sentiment_data('Sentiment-Analysis/Sentences_75Agree_sample.txt')\n",
    "print(\"Dataset loaded successfully:\")\n",
    "print(data_df.head())\n",
    "\n",
    "\n",
    "# ---\n",
    "# #### Task 2: Text Preprocessing with NLTK\n",
    "#\n",
    "# This task is inspired by the techniques shown in `LabText_1.ipynb`.\n",
    "# We will clean the text by removing punctuation, converting to lowercase, stemming, and removing stopwords.\n",
    "# ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a single text entry.\n",
    "    \"\"\"\n",
    "    # Inspired by: LabText_1.ipynb for using NLTK for stemming and stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    # Tokenize the text\n",
    "    tokens = re.split('\\\\W+', text)\n",
    "    # Remove stopwords and perform stemming\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "print(\"\\nPreprocessing text data...\")\n",
    "data_df['cleaned_text'] = data_df['text'].apply(preprocess_text)\n",
    "print(\"Text preprocessing complete.\")\n",
    "print(data_df.head())\n",
    "\n",
    "\n",
    "# ---\n",
    "# ### Section 2: Exploratory Data Analysis\n",
    "#\n",
    "# Let's explore the data to understand the distribution of sentiments and sentence lengths.\n",
    "# ---\n",
    "\n",
    "# #### Task 3: Data Visualization\n",
    "#\n",
    "# We will visualize the sentiment distribution and the length of the sentences.\n",
    "# Inspired by: Course_1_-_Quickstart.ipynb for creating visualizations with matplotlib and seaborn.\n",
    "# ---\n",
    "\n",
    "# Sentiment Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', data=data_df)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Sentence Length Distribution\n",
    "data_df['text_length'] = data_df['cleaned_text'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_df['text_length'], bins=50)\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.xlabel('Length of Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---\n",
    "# ### Section 3: Building a Sentiment Analysis Model with a CNN\n",
    "#\n",
    "# We'll use a 1D Convolutional Neural Network (CNN) for this task.\n",
    "# This technique is adapted from `Course_4_-_CNNs.ipynb.ipynb`, which uses CNNs for image analysis. Here, we apply the same concept to text data.\n",
    "# ---\n",
    "\n",
    "# #### Task 4: Prepare Data for the CNN Model\n",
    "#\n",
    "# We need to tokenize the text and convert it into sequences that our model can understand.\n",
    "# Inspired by: Course_3_-_Neural_Nets.ipynb for data preparation for a Keras model.\n",
    "# ---\n",
    "\n",
    "# Label encoding\n",
    "data_df['sentiment_encoded'] = data_df['sentiment'].apply(lambda x: 1 if x == 'positive' else (0 if x == 'negative' else 2))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_df['cleaned_text'], data_df['sentiment_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_seq_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "\n",
    "# ---\n",
    "# #### Task 5: Build and Train the CNN Model\n",
    "#\n",
    "# Here we define and train our 1D CNN model.\n",
    "# The model architecture is inspired by the CNN model in `Course_4_-_CNNs.ipynb.ipynb`.\n",
    "# ---\n",
    "\n",
    "def create_cnn_model(vocab_size, max_seq_length):\n",
    "    \"\"\"\n",
    "    Creates a 1D CNN model for sentiment analysis.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 128, input_length=max_seq_length),\n",
    "        # Inspired by: Course_4_-_CNNs.ipynb.ipynb for using Convolutional layers\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        # Inspired by: Course_3_-_Neural_Nets.ipynb for Dense and Dropout layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "cnn_model = create_cnn_model(vocab_size, max_seq_length)\n",
    "print(\"\\nTraining the CNN model...\")\n",
    "history = cnn_model.fit(X_train_padded, y_train_cat, epochs=5, validation_split=0.2, batch_size=32, verbose=1)\n",
    "print(\"CNN model training complete.\")\n",
    "\n",
    "# ---\n",
    "# #### Task 6: Evaluate the CNN Model\n",
    "# Inspired by: Course_3_-_Neural_Nets.ipynb and Course_2_-_RF_KNN.ipynb for model evaluation techniques.\n",
    "# ---\n",
    "\n",
    "# Evaluate on the test set\n",
    "loss, accuracy = cnn_model.evaluate(X_test_padded, y_test_cat, verbose=0)\n",
    "print(f'\\nCNN Model Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = cnn_model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nCNN Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=['negative', 'positive', 'neutral']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'positive', 'neutral'], yticklabels=['negative', 'positive', 'neutral'])\n",
    "plt.title('CNN Model Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# ---\n",
    "# ### Section 4: Baseline Model with Random Forest\n",
    "#\n",
    "# To better understand the performance of our CNN, we will compare it with a Random Forest classifier.\n",
    "# This entire section is inspired by `Course_2_-_RF_KNN.ipynb`.\n",
    "# ---\n",
    "\n",
    "# #### Task 7: Prepare Data and Train the Random Forest Model\n",
    "# ---\n",
    "\n",
    "# Inspired by: Course_2_-_RF_KNN.ipynb for using scikit-learn for feature extraction and model training.\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "print(\"\\nTraining the Random Forest model...\")\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "print(\"Random Forest model training complete.\")\n",
    "\n",
    "# ---\n",
    "# #### Task 8: Evaluate the Random Forest Model\n",
    "# Inspired by: Course_2_-_RF_KNN.ipynb for evaluating a scikit-learn model.\n",
    "# ---\n",
    "y_pred_rf = rf_model.predict(X_test_vec)\n",
    "\n",
    "print(\"\\nRandom Forest Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['negative', 'positive', 'neutral']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', xticklabels=['negative', 'positive', 'neutral'], yticklabels=['negative', 'positive', 'neutral'])\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
